import streamlit as st
import time

# Configure the Streamlit page with title and icon
st.set_page_config(page_title="RAG Demo", page_icon="ðŸ¤–")

# Display the main title and description of the application
st.title("ðŸ¤– RAG Question Answering")
st.markdown("""
This is a dummy RAG interface. Ask a question about your documents, and the system will (mock) retrieve relevant sections and generate an answer.
""")

# Initialize the session state to store chat messages
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display all previous messages from the chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # Show citations/sources if available for assistant messages
        if "citations" in message:
            with st.expander("Sources"):
                for source in message["citations"]:
                    st.write(f"- {source}")

# Handle user input and process the question
if prompt := st.chat_input("Ask a question about your documents..."):
    # Store and display the user's message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generate and display the assistant's response
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        # Mock RAG response - simulates what a real LLM would generate
        dummy_answer = f"Based on the documents provided, here is a mock answer to your question: '{prompt}'. In a real RAG system, this would be generated by an LLM using retrieved document chunks."
        dummy_citations = [
            "Source: rag-project-overview.md (Section: Success Criteria)",
            "Source: RAG_Mastery_Building_Dynamic_AI.pdf (Page 12)"
        ]

        # Simulate streaming response for realistic chat experience
        for chunk in dummy_answer.split():
            full_response += chunk + " "
            time.sleep(0.05)
            message_placeholder.markdown(full_response + "â–Œ")

        message_placeholder.markdown(full_response)

        # Display the source citations in an expandable section
        with st.expander("Sources"):
            for source in dummy_citations:
                st.write(f"- {source}")

        # Store the assistant's response with citations in session state
        st.session_state.messages.append({
            "role": "assistant",
            "content": full_response,
            "citations": dummy_citations
        })
